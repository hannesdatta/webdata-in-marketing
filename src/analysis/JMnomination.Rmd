---
title: 'Tables and Figures for "Fields of Gold"'
author: "Hannes Datta"
date: "02/08/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(ggplot2)
library(stringr)
library(dplyr)
library(knitr)
library(tidyverse)
library(stargazer)

coding <- fread('../../gen/prepare/output/coding.csv')
cites_yearly <- fread('../../gen/prepare/output/cites_per_paper_per_year.csv')
cites_summary <- fread('../../gen/prepare/output/cites_per_paper.csv')

coding[grepl('Datta', authors, ignore.case=T)]

coding[wos_id=='WOS000400895000001']

# same year
relevant_papers = coding[year==2017&journal=='JM']

anal_papers = cites_yearly[wos_match%in%relevant_papers$wos_id]

anal_papers[, our_paper := wos_match=='WOS000400895000001']

# all
'our_paper'

cites = fread('../../gen/prepare/temp/wos_citations.csv')
tmp=data.frame(table(cites[wos_id=='WOS:000400895000001']$journal))
tmp$rank=rank(-tmp$Freq)
tmp = data.table(tmp)
sum(tmp[rank<=10]$Freq)/sum(tmp$Freq)
nrow(tmp)



 

tmp = anal_papers[, lapply(.SD, mean), by = c('year', 'our_paper'), .SDcols= grep('cites', colnames(anal_papers),ignore.case=T)]

plot(tmp$year, tmp$Ncites_ft50)

# distribution of papers
summary(anal_papers)

# top papers

anal_papers[year==2021, total_cites_rank := rank(-Ncites_total_cum)]
anal_papers[, total_cites_rank := unique(total_cites_rank[!is.na(total_cites_rank)]), by = 'wos_match']

# ranked
setkey(anal_papers, wos_match)
setkey(coding, wos_id)

anal_papers[coding, ':=' (authors=i.authors, title=substr(i.title_wos,1,50))]

anal_papers[year==2021&total_cites_rank<=10]

# ranks
ranks = anal_papers[year==2021, list(wos_match=wos_match,
                                     total_cites = rank(-Ncites_total_cum),
                                     ft50 = rank(-Ncites_ft50_cum),
                                     marketing = rank(-Ncites_marketing_cum),
                                     nonmarketing = rank(-Ncites_notmarketing_cum),
                                     top4_marketing = rank(-Ncites_jm_cum-Ncites_jmr_cum-Ncites_mktsci_cum-Ncites_jcr_cum)
                                     )]
setkey(ranks, wos_match)
setkey(coding, wos_id)

ranks[coding, ':=' (authors=i.authors, title=substr(i.title_wos,1,50))]
setorderv(ranks, 'total_cites', order=1L)

ranks[1:10]


# our cites versus the cites of the others

# top 10
top10wos = ranks[1:5]$wos_match

tmp = anal_papers[wos_match%in%top10wos]
setkey(tmp, wos_match)
setkey(coding, wos_id)

tmp[coding, ':=' (authors=i.authors, title=substr(i.title_wos,1,80))]

tmp[, paper:=paste0(substr(authors,1,20), substr(title,1,20))]

ggplot(data=tmp, aes(x=year, y=Ncites_total_cum, group=paper))+
  geom_line(aes(linetype=paper))+
  geom_point() + ggtitle('Total cites')

ggplot(data=tmp, aes(x=year, y=Ncites_ft50_cum, group=paper))+
  geom_line(aes(linetype=paper))+
  geom_point() + ggtitle('FT50 cites') # highest increase in top 50 cites

ggplot(data=tmp, aes(x=year, y=Ncites_notmarketing_cum, group=paper))+
  geom_line(aes(linetype=paper))+
  geom_point() + ggtitle('Non-marketing cites')

ggplot(data=tmp, aes(x=year, y=Ncites_marketing_cum, group=paper))+
  geom_line(aes(linetype=paper))+
  geom_point() + ggtitle('Marketing cites')

# most OUTSIDE of marketing



```

```{r}
coding[, first_web:=min(year[web==T]),by=c('journal')]

growth = rbindlist(lapply(c('first','last'), function(.half) {
  coding[, filter:=year>=first_web & year<=2012]
  if (.half=='last') coding[, filter:=year>=first_web &year>2012]
  
  gr = coding[filter==T, list(publications = .N, years = uniqueN(year),
                       publications_web = length(which(web==T)),
              first_pub=unique(first_web)),
                by=c('journal')][, half:=.half]
  return(gr)
}))
  
growth[, ':=' (avgoutput=publications/years, avgoutput_web = publications_web/years)]

gr=growth[, list(growth_all = avgoutput[half=='last']/avgoutput[half=='first'],
              growth_web = avgoutput_web[half=='last']/avgoutput_web[half=='first']), by = c('journal')]

setkey(gr, journal)
setkey(coding, journal)
coding[gr, ':=' (growth_all = i.growth_all,
                 growth_web = i.growth_web,
                 growth_index = i.growth_web/i.growth_all)]


aggs <- rbindlist(lapply(c('journal','no'), function(.by) {
  coding[, no:='total']
  
tmp = coding[, list(N=length(which(web==T)), 
                    first_web_paper = min(year[web==T]),
                    share = round(100*mean(web[year>=first_web]==T),2),
                    share2020 = round(100*mean(web[year>=first_web&year%in%2020]==T),2),
                    
                    avg_papers_per_year = length(which(web==T))/(2020-min(year[web==T])+1),
             
                   growth = mean(growth_web),
                   growthnotweb = mean(growth_all),
                   growth_index = mean(growth_index),
                  share_juniors=round(100 * length(which(author_assistant_professor+author_post_doc+author_phd>0))/length(which(web==T)),0),
                  
                  cites_per_year = median(ncites_total_avg[web==T]),
                  cites_per_yearnonweb = median(ncites_total_avg[web==F], na.rm=T),
                  
                  cites_per_year_indexed = median(ncites_total_avg[web==T])/median(ncites_total_avg[web==F],na.rm=T)), by = .by]
if (.by=='no') setnames(tmp, 'no', 'journal')
  tmp
}))

journals = c('JM','JMR','JCR','JCP','MktSci')
tmp2 = dcast(melt(aggs, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable', journals,  'total'))


kable(tmp2,digits=2, caption = 'Web data in marketing research (output & cites)')

```


```{r}

topx=data.table(coding[web==T])

setorderv(topx, 'ncites_total_avg', order=-1L)
topx[, share_ft50 := 100*ncites_ft50/ncites_total]
topx[, share_outside := 100*ncites_notmarketing/ncites_total]
topx[, rank:=1:.N]
     
kable(topx[1:30, c('rank','authors','year', 'ncites_total_avg'),with=F],digits=1, caption = "Top papers in Marketing using Web Data")

```

```{r}

platform_count = coding[web==T, list(
  any_api = any(api==1),
  platforms=unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,',')))))),
                          by = c('doi', 'journal')]

tmp = platform_count[, list(JM=uniqueN(doi[journal=='JM']),
                            JMR=uniqueN(doi[journal=='JMR']),
                            JCR=uniqueN(doi[journal=='JCR']),
                            JCP=uniqueN(doi[journal=='JCP']),
                            MktSci=uniqueN(doi[journal=='MktSci']),
                            N=uniqueN(doi)
                            ),by=c('platforms')]
setorderv(tmp, 'N', order=-1L)

tmp[, text:=paste0(str_to_title(platforms), ' (', N, ')')]
tmp[, rank:=1:.N]

kable(tmp[1:30],caption = 'Top Sources')

```


```{r}

setkey(cites_summary, wos_match)
setkey(coding, wos_id)

cites_summary[coding, ':=' (journal=i.journal, issue=i.issue, volume=i.vol,
             web=i.web, pubyear = i.year)]

cites_summary[, fe:=paste0(journal,'_', volume, '_', issue)]

cnt=0
for (.fe in unique(cites_summary$fe)) {
  cnt=cnt+1
  cites_summary[, paste0('fe_', cnt):=ifelse(fe==.fe,1,0)-mean(fe==.fe)]
}

for (.j in unique(cites_summary$journal)) {
  cites_summary[, paste0('web_', .j):=web*ifelse(journal==.j,1,0)]
}

cites_summary[, fe:=paste0(journal,'_', volume, '_', issue)]


m1<-lm(log(Ncites_total_avg+1)~1+web+as.factor(fe), data=cites_summary)
m2<-lm(log(Ncites_total_avg+1)~1+web_JM + web_JCP + web_JCR + web_JMR + web_MktSci+as.factor(fe), data=cites_summary)



stargazer(list(marketing=m1,controls=m2),type='text', keep='web|intercept', title = 'Citation Analysis')



```


```{r}

top_platforms= platform_count[, list(N=.N),by=c('journal','platforms')]
top_platforms[, marketshare:=(N/sum(N)),by=c('journal')]
top_platforms[, rank:=rank(-marketshare, ties.method='min'),by=c('journal')]

setkey(platform_count, journal, platforms)
setkey(top_platforms, journal, platforms)

platform_count[top_platforms, platform_rank:=i.rank]
platform_count[, ':=' (top3 = any(platform_rank<=3),
                       top5 = any(platform_rank<=5),
                       n_sources = uniqueN(platforms)),by=c('doi')]

setkey(platform_count, doi)
setkey(coding, doi)

coding[platform_count, ':=' (top3_platform = i.top3,
                             top5_platform = i.top5,
                             n_sources=i.n_sources)]




top_platforms[, text:=paste0(str_to_title(platforms), ' (', N, ')')]
setorder(top_platforms, journal, rank)

top_tmp =top_platforms[, list(Sources=paste0(text[rank<=10],collapse=', ')), by = c('journal')]
top_tmp=top_tmp[match(journals, top_tmp$journal),]


aggs <- rbindlist(lapply(c('journal','no'), function(.by) {
  coding[, no:='total']
  
tmp = coding[, list(N=length(which(web==T)), 
                    share_top5 = round(mean(100*top5_platform[web==T]),0),
                 
                    share_single = round(100*length(which(n_sources[web==T]==1))/length(which(web==T)),0), 
                   share_api = round(100*length(which(api==1&scraped==0))/length(which(web==T)),0),
                   
                   share_northamerica = round(100*length(which(grepl('US|canada', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),0),
                  share_europe = round(100*length(which(grepl('eu|ger|uk|germany|italy|sweden|france|austria|switz|denmark|netherl|poland|luxem|belgium|czech', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),0),
                  
                  #top_keywords_first= get_n_keywords(as.character(keywords_author[web==T])),
                #  top_keywords_late= get_n_keywords(as.character(keywords_author[web==T & year >2012])),
                  
                  share_textual = round(100*length(which(textual==1))/length(which(web==T)),0),
                  share_numeric = round(100*length(which(numeric==1))/length(which(web==T)),0),
                  share_images = round(100 *length(which(images==1))/length(which(web==T)),0),
                  share_video = round(100*length(which(video==1))/length(which(web==T)),0),
               #   share_aggregator = round(100 * length(which(is_aggregator))/length(which(web==T)),0),
                  
                  
                  share_collected_once = round(100*length(which(live_scraper==F&web==T))/length(which(web==T)),0)), by = .by]
if (.by=='no') setnames(tmp, 'no', 'journal')
  tmp
}))

journals = c('JM','JMR','JCR','JCP','MktSci')
tmp2 = dcast(melt(aggs, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable', journals,  'total'))


kable(tmp2,digits=2, caption = 'Data Selection and Extraction')

```


```{r}
# citation figure

setkey(cites_yearly, wos_match)
setkey(coding, wos_id)

cites_yearly[coding, ':=' (journal=i.journal, issue=i.issue, volume=i.vol,
                           web=i.web, pubyear = i.year)]

for (i in 1:18) {
  cites_yearly[, paste0('years_since_', i):= (year-pubyear+1)==i & web == F]
  cites_yearly[, paste0('years_websince_', i):= (year-pubyear+1)==i & web == T]
}

cites_yearly[, fe:=paste0(journal,'_', volume, '_', issue)]

cnt=0
for (.fe in unique(cites_yearly$fe)) {
  cnt=cnt+1
  cites_yearly[, paste0('fe_', cnt):=ifelse(fe==.fe,1,0)-mean(fe==.fe)]
}


X = as.matrix(cites_yearly[, grep('years_since_[0-9]|years_websince|fe_', colnames(cites_yearly), value=T),with=F])

m1<-lm(cites_yearly$Ncites_total_cum ~ -1 + X)


# extract coefs
extracted_coefs = data.table(summary(m1)$coefficients)
extracted_coefs[, vars:=rownames(summary(m1)$coefficients)]



extracted_coefs <- extracted_coefs[grepl('year', vars)]
extracted_coefs[, web:=grepl('web', vars)]
extracted_coefs[, year:= as.numeric(gsub('.*_', '', vars))]



extracted_coefs[, web_label:=as.factor(ifelse(web==T, 'With web data', 'Without web data'))]

extracted_coefs %>% filter(year<=15) %>% 
  ggplot( aes(x=year, y=Estimate, group=web_label, linetype = web_label)) +
  geom_line()+geom_point() + 
  geom_errorbar(aes(ymin=Estimate-1.69*`Std. Error`, ymax=Estimate+1.69*`Std. Error`), width=.2,
                position=position_dodge(0.05)) + xlab('Years Since Publication') + ylab('Total Number of Citations')+scale_fill_manual(values = c("gray30", "grey85", "gray60", "gray40", "gray1")) +  theme_bw() + guides(linetype = guide_legend(reverse=F)) + 
  labs(linetype='Use of Web Data')+
  theme(plot.caption = element_text(hjust = 0.2, face = "italic"),
        axis.title = element_text(face="bold", size = "13"),
        legend.title = element_text(face="bold", size = "13"),
        axis.text.x = element_text(size="8.5")) + 
  guides(fill = guide_legend(reverse=T)) +theme(legend.position="bottom")


dir.create('../../gen/analysis/output', recursive = T)

ggsave('../../gen/analysis/output/citations.png',width=6,height=5)


```


```{r}
# JOURNAL TIME GRAPH

timing = coding[web==T, list(N=.N),by=c('journal', 'year')]
coding[, first_web:=min(year[web==T]),by=c('journal')]
timing_share = coding[, list(share=mean(web[year>=first_web])), by= 'year']

setorder(timing_share, year)

setkey(timing_share, year)
setkey(timing, year)
timing[timing_share, share:=100*i.share]

timing %>%
  mutate(journal = factor(journal, levels = c("JCP", "JCR", "MktSci", "JMR", "JM"))) %>%
  ggplot(aes(year, N)) +
  geom_bar(stat = 'identity', aes(fill=journal)) +
  geom_line(aes(x=year, y=share), colour="red", size = 1.5, linetype=1) + 
  labs(fill = "Journal",
       x = "Year of Publication",
       y = "Number of Articles") +
  scale_fill_manual(values = c("gray30", "grey85", "gray60", "gray40", "gray5")) +
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0.2, face = "italic"),
        axis.title = element_text(face="bold", size = "13"),
        legend.title = element_text(face="bold", size = "13"),
        axis.text.x = element_text(size="8.5")) +
  scale_x_discrete(limits=c(2004:2020)) +  
  scale_y_continuous(
    "Number of Articles", 
    sec.axis = sec_axis(~ . * 1.20, name = "Use of Web Data (in %)")
  ) + 
  guides(fill = guide_legend(reverse=T)) +theme(legend.position="bottom")

ggsave('../../gen/analysis/output/barchart.png',width=10,height=6)


```
