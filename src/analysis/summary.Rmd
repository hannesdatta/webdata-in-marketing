---
title: "Summary stats"
author: "Hannes Datta"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(ggplot2)
library(stringr)
library(dplyr)
library(knitr)
library(tidyverse)

load(file= '../../gen/analysis/temp/citation_database.RData')

# dump to Johannes

fwrite(coding, '../../gen/analysis/output/coding.csv')
fwrite(cites_summary, '../../gen/analysis/output/cites_per_paper.csv')
fwrite(cites_yearly, '../../gen/analysis/output/cites_per_paper_per_year.csv')



```

# Overview

```{r}
coding[, first_web:=min(year[web==T]),by=c('journal')]

growth = rbindlist(lapply(c('first','last'), function(.half) {
  coding[, filter:=year>=first_web & year<=2012]
  if (.half=='last') coding[, filter:=year>=first_web &year>2012]
  
  gr = coding[filter==T, list(publications = .N, years = uniqueN(year),
                       publications_web = length(which(web==T)),
              first_pub=unique(first_web)),
                by=c('journal')][, half:=.half]
  return(gr)
}))
  
growth[, ':=' (avgoutput=publications/years, avgoutput_web = publications_web/years)]

gr=growth[, list(growth_all = avgoutput[half=='last']/avgoutput[half=='first'],
              growth_web = avgoutput_web[half=='last']/avgoutput_web[half=='first']), by = c('journal')]

setkey(gr, journal)
setkey(coding, journal)
coding[gr, ':=' (growth_all = i.growth_all,
                 growth_web = i.growth_web,
                 growth_index = i.growth_web/i.growth_all)]


aggs <- rbindlist(lapply(c('journal','no'), function(.by) {
  coding[, no:='total']
  
tmp = coding[, list(N=length(which(web==T)), 
                    first_web_paper = min(year[web==T]),
                    share = round(100*mean(web[year>=first_web]==T),2),
                    share2020 = round(100*mean(web[year>=first_web&year%in%2020]==T),2),
                    
                    avg_papers_per_year = length(which(web==T))/(2020-min(year[web==T])+1),
             
                   growth = mean(growth_web),
                   growthnotweb = mean(growth_all),
                   growth_index = mean(growth_index),
                  share_juniors=round(100 * length(which(author_assistant_professor+author_post_doc+author_phd>0))/length(which(web==T)),0),
                  
                  cites_per_year = median(ncites_total_avg[web==T]),
                  cites_per_yearnonweb = median(ncites_total_avg[web==F], na.rm=T),
                  
                  cites_per_year_indexed = median(ncites_total_avg[web==T])/median(ncites_total_avg[web==F],na.rm=T)), by = .by]
if (.by=='no') setnames(tmp, 'no', 'journal')
  tmp
}))

journals = c('JM','JMR','JCR','JCP','MktSci')
tmp2 = dcast(melt(aggs, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable', journals,  'total'))


kable(tmp2,digits=2, caption = 'Web data in marketing research')

```

```{r}
# JOURNAL TIME GRAPH
timing = coding[web==T, list(N=.N),by=c('journal', 'year')]
coding[, first_web:=min(year[web==T]),by=c('journal')]
timing_share = coding[, list(share=mean(web[year>=first_web])), by= 'year']

setorder(timing_share, year)

setkey(timing_share, year)
setkey(timing, year)
timing[timing_share, share:=100*i.share]


```


# Impactful articles

```{r}

topx=data.table(coding[web==T])

setorderv(topx, 'ncites_total_avg', order=-1L)
topx[, share_ft50 := 100*ncites_ft50/ncites_total]
topx[, share_outside := 100*ncites_notmarketing/ncites_total]
topx[, rank:=1:.N]
     
kable(topx[1:30, c('rank','authors','year', 'ncites_total_avg'),with=F],digits=1)

```

# Sources

```{r}

platform_count = coding[web==T, list(
  any_api = any(api==1),
  platforms=unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,',')))))),
                          by = c('doi', 'journal')]

tmp = platform_count[, list(JM=uniqueN(doi[journal=='JM']),
                            JMR=uniqueN(doi[journal=='JMR']),
                            JCR=uniqueN(doi[journal=='JCR']),
                            JCP=uniqueN(doi[journal=='JCP']),
                            MktSci=uniqueN(doi[journal=='MktSci']),
                            N=uniqueN(doi)
                            ),by=c('platforms')]
setorderv(tmp, 'N', order=-1L)

tmp[, text:=paste0(str_to_title(platforms), ' (', N, ')')]
tmp[, rank:=1:.N]

kable(tmp[1:30])

```


###
```{r}
# citation figure

setkey(cites_yearly, wos_match)
setkey(coding, wos_id)

cites_yearly[coding, ':=' (journal=i.journal, issue=i.issue, volume=i.vol,
             web=i.web, pubyear = i.year)]

for (i in 1:18) {
  cites_yearly[, paste0('years_since_', i):= (year-pubyear+1)==i & web == F]
  cites_yearly[, paste0('years_websince_', i):= (year-pubyear+1)==i & web == T]
}

cites_yearly[, fe:=paste0(journal,'_', volume, '_', issue)]

cnt=0
for (.fe in unique(cites_yearly$fe)) {
  cnt=cnt+1
  cites_yearly[, paste0('fe_', cnt):=ifelse(fe==.fe,1,0)-mean(fe==.fe)]
}

#X = as.matrix(cites_yearly[, grep('years_since_[0-9]|years_websince|fe_', colnames(cites_yearly), value=T),with=F])

#m1<-lm(cites_yearly$Ncites_total_cum ~ -1 + X)



# extract coefs
extracted_coefs = data.table(summary(m1)$coefficients)
extracted_coefs[, vars:=rownames(summary(m1)$coefficients)]



extracted_coefs <- extracted_coefs[grepl('year', vars)]
extracted_coefs[, web:=grepl('web', vars)]
extracted_coefs[, year:= as.numeric(gsub('.*_', '', vars))]



extracted_coefs[, web_label:=as.factor(ifelse(web==T, 'Web data', 'No web data'))]
levels(extracted_coefs$web_label)=c('With web data', 'Without web data')


extracted_coefs %>% filter(year<=15) %>% 
  ggplot( aes(x=year, y=Estimate, group=web_label, linetype = web_label)) +
    geom_line()+geom_point() + 
  geom_errorbar(aes(ymin=Estimate-1.69*`Std. Error`, ymax=Estimate+1.69*`Std. Error`), width=.2,
                 position=position_dodge(0.05)) + xlab('Years Since Publication') + ylab('Total Number of Citations')+scale_fill_manual(values = c("gray30", "grey85", "gray60", "gray40", "gray1")) +  theme_bw() + guides(linetype = guide_legend(reverse=F)) + 
  labs(linetype='Use of Web Data')+
  theme(plot.caption = element_text(hjust = 0.2, face = "italic"),
        axis.title = element_text(face="bold", size = "13"),
        legend.title = element_text(face="bold", size = "13"),
        axis.text.x = element_text(size="8.5")) + 
  guides(fill = guide_legend(reverse=T)) +theme(legend.position="bottom")


dir.create('../../gen/analysis/output', recursive = T)


ggsave('../../gen/analysis/output/citations.png',width=6,height=5)



timing %>%
  mutate(journal = factor(journal, levels = c("JCP", "JCR", "MktSci", "JMR", "JM"))) %>%
  ggplot(aes(year, N)) +
  geom_bar(stat = 'identity', aes(fill=journal)) +
  geom_line(aes(x=year, y=share), colour="red", size = 1.5, linetype=1) + 
  labs(fill = "Journal",
       x = "Year of Publication",
       y = "Number of Articles") +
  scale_fill_manual(values = c("gray30", "grey85", "gray60", "gray40", "gray5")) +
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0.2, face = "italic"),
        axis.title = element_text(face="bold", size = "13"),
        legend.title = element_text(face="bold", size = "13"),
        axis.text.x = element_text(size="8.5")) +
  scale_x_discrete(limits=c(2004:2020)) +  
  scale_y_continuous(
    "Number of Articles", 
    sec.axis = sec_axis(~ . * 1.20, name = "Use of Web Data (in %)")
  ) + 
  guides(fill = guide_legend(reverse=T)) +theme(legend.position="bottom")


ggsave('../../gen/analysis/output/barchart.png',width=6,height=5)
ggsave('../../gen/analysis/output/barchart2.png',width=10,height=6)


```

```{r}


setkey(cites_summary, wos_match)
setkey(coding, wos_id)

cites_summary[coding, ':=' (journal=i.journal, issue=i.issue, volume=i.vol,
             web=i.web, pubyear = i.year)]

cites_summary[, fe:=paste0(journal,'_', volume, '_', issue)]

cnt=0
for (.fe in unique(cites_summary$fe)) {
  cnt=cnt+1
  cites_summary[, paste0('fe_', cnt):=ifelse(fe==.fe,1,0)-mean(fe==.fe)]
}

for (.j in unique(cites_summary$journal)) {
  cites_summary[, paste0('web_', .j):=web*ifelse(journal==.j,1,0)]
}

cites_summary[, fe:=paste0(journal,'_', volume, '_', issue)]


m1<-lm(log(Ncites_total_avg+1)~1+web+as.factor(fe), data=cites_summary)
m2<-lm(log(Ncites_total_avg+1)~1+web_JM + web_JCP + web_JCR + web_JMR + web_MktSci+as.factor(fe), data=cites_summary)


library(stargazer)
stargazer(list(marketing=m1,controls=m2),type='text', keep='web|intercept')



```

```{r}

top_platforms= platform_count[, list(N=.N),by=c('journal','platforms')]
top_platforms[, marketshare:=(N/sum(N)),by=c('journal')]
top_platforms[, rank:=rank(-marketshare, ties.method='min'),by=c('journal')]

setkey(platform_count, journal, platforms)
setkey(top_platforms, journal, platforms)

platform_count[top_platforms, platform_rank:=i.rank]
platform_count[, ':=' (top3 = any(platform_rank<=3),
                       top5 = any(platform_rank<=5),
                       n_sources = uniqueN(platforms)),by=c('doi')]

setkey(platform_count, doi)
setkey(coding, doi)

coding[platform_count, ':=' (top3_platform = i.top3,
                             top5_platform = i.top5,
                             n_sources=i.n_sources)]




top_platforms[, text:=paste0(str_to_title(platforms), ' (', N, ')')]
setorder(top_platforms, journal, rank)

top_tmp =top_platforms[, list(Sources=paste0(text[rank<=10],collapse=', ')), by = c('journal')]
top_tmp=top_tmp[match(journals, top_tmp$journal),]


aggs <- rbindlist(lapply(c('journal','no'), function(.by) {
  coding[, no:='total']
  
tmp = coding[, list(N=length(which(web==T)), 
                    share_top5 = round(mean(100*top5_platform[web==T]),0),
                 
                    share_single = round(100*length(which(n_sources[web==T]==1))/length(which(web==T)),0), 
                   share_api = round(100*length(which(api==1&scraped==0))/length(which(web==T)),0),
                   
                   share_northamerica = round(100*length(which(grepl('US|canada', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),0),
                  share_europe = round(100*length(which(grepl('eu|ger|uk|germany|italy|sweden|france|austria|switz|denmark|netherl|poland|luxem|belgium|czech', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),0),
                  
                  #top_keywords_first= get_n_keywords(as.character(keywords_author[web==T])),
                #  top_keywords_late= get_n_keywords(as.character(keywords_author[web==T & year >2012])),
                  
                  share_textual = round(100*length(which(textual==1))/length(which(web==T)),0),
                  share_numeric = round(100*length(which(numeric==1))/length(which(web==T)),0),
                  share_images = round(100 *length(which(images==1))/length(which(web==T)),0),
                  share_video = round(100*length(which(video==1))/length(which(web==T)),0),
               #   share_aggregator = round(100 * length(which(is_aggregator))/length(which(web==T)),0),
                  
                  
                  share_collected_once = round(100*length(which(live_scraper==F&web==T))/length(which(web==T)),0)), by = .by]
if (.by=='no') setnames(tmp, 'no', 'journal')
  tmp
}))

journals = c('JM','JMR','JCR','JCP','MktSci')
tmp2 = dcast(melt(aggs, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable', journals,  'total'))


kable(tmp2,digits=2, caption = 'Data Selection and Extraction')

                  #share_US_any = length(which(grepl('US', `Geography_Country (or worldwide)`)))/.N
                  

```

```{r}

get_n_keywords = function(x, topx=3) {
 res=gsub('-', ' ', as.character(x))
  
 res= (str_trim(tolower(unlist(strsplit(res, ';')))))
 res=table(res)
 res=rev(res[order(res)])
 return(paste0(names(res)[1:topx], collapse=', '))
}
  

# -uniq platforms / number of papers (excluding "outliers")
# - source concentration top 3 share, top 5 share (herfindahl). 
# - names of top 3 sources

```

## Concentration

```{r}
head(coding$keywords_author)

cnt = tolower(str_trim(unlist(strsplit(coding[web==T]$keywords_author,';'))))
tmp=data.table(keyword=cnt)[, list(.N),by=c('keyword')]

setorder(tmp, N)

```

### Analysis
```{r}
# Overall
clean_data_source <- function(x) {
  x[grepl('googletrends', x, ignore.case=T)]<- 'Google Trends'
x[grepl('imdb', x, ignore.case=T)]<- 'IMDB'
x[grepl('amazon', x, ignore.case=T)]<- 'Amazon'
x[grepl('bn[.]com', x, ignore.case=T)]<- 'Barnes & Nobles'

  x
}
```



```{r}

tmp = platform_count[, list(N=uniqueN(doi)),by=c('platforms')]
setorderv(tmp, 'N', order=-1L)


tmp[, text:=paste0(str_to_title(platforms), ' (', N, ')')]
tmp[, rank:=1:.N]

tmp[1:30, c('rank','text'),with=F]

coding[web==T, list(1-mean(api))]

```



```{r}
  
# Notes:
# 
# focal variables of interest
# --> add avg. number of platforms
# add platform concentration (make clean column)
# keyword distrubtion + keyword concentration index
# Hannes checks web of science to compare author juniors
# relative impact (impact of paper, relative to all papers in the journal)
# Highest cited papers, per year + references
# Illustrative papers, highest-impact research, by year


# -PhD/Assistant/Postdocs
kable(top_tmp, caption = 'Top 10 Sources by Journal')
```



