---
title: "Summary stats"
author: "Hannes Datta"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(ggplot2)
library(stringr)
library(bibliometrix)

load(file= '../../gen/analysis/temp/citation_database.RData')
```

### Analysis
```{r}
# Overall
clean_data_source <- function(x) {
  x[grepl('googletrends', x, ignore.case=T)]<- 'Google Trends'
x[grepl('imdb', x, ignore.case=T)]<- 'IMDB'
x[grepl('amazon', x, ignore.case=T)]<- 'Amazon'
x[grepl('bn[.]com', x, ignore.case=T)]<- 'Barnes & Nobles'

  x
}
```

```{r}

get_n_keywords = function(x, topx=3) {
 res=gsub('-', ' ', as.character(x))
  
 res= (str_trim(tolower(unlist(strsplit(res, ';')))))
 res=table(res)
 res=rev(res[order(res)])
 return(paste0(names(res)[1:topx], collapse=', '))
}
  
first_platforms = coding[web==T, list(platforms=unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,',')))))),
                          by = c('doi', 'year')]
first_platforms[, first:=year==min(year),by=c('platforms')]
first_platforms_agg = first_platforms[, list(first=any(first)),by=c('doi')]

setkey(first_platforms_agg, doi)
setkey(coding, doi)
coding[first_platforms_agg, first_time:=i.first]

tmp = coding[, list(N=length(which(web==T)), first_web_paper = min(year[web==T]),
                  avg_papers_per_year = length(which(web==T))/(2020-min(year[web==T])+1),
                  share_juniors=length(which(author_assistant_professor+author_post_doc+author_phd>0))/length(which(web==T)),
                  
                  cites_per_year = mean(tcperyear[web==T]),
                  cites_per_year_indexed = mean(tcperyear[web==T])/mean(tcperyear),
                  
                  number_platforms = length(unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,',')))))),
                  platforms_per_paper = length(unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,','))))))/length(which(web==T)),
                  share_firsttime_platforms = mean(first_time[web==T]),
                  share_worldwide = length(which(grepl('worldwide', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),
                  share_northamerica = length(which(grepl('US|canada', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),
                  share_europe = length(which(grepl('eu|ger|uk|germany|italy|sweden|france|austria|switz|denmark|netherl|poland|luxem|belgium|czech', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  share_asia = length(which(grepl('china|korea|japan', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  share_other = length(which(grepl('N[/]A|English[-]speaking', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  
                  top_keywords= get_n_keywords(as.character(keywords_author[web==T])),
                  
                 
                  share_textual = length(which(textual==1))/length(which(web==T)),
                  share_numeric = length(which(numeric==1))/length(which(web==T)),
                  share_images = length(which(images==1))/length(which(web==T)),
                  share_video = length(which(video==1))/length(which(web==T)),
                  
                  share_scraped = length(which(scraped==1&api==0))/length(which(web==T)),
                  share_api = length(which(api==1&scraped==0))/length(which(web==T)),
                  share_scraped_api = length(which(scraped==1&api==1))/length(which(web==T)),
                  
                  share_live = length(which(live_scraper==T))/length(which(web==T))
                  
                  #share_US_any = length(which(grepl('US', `Geography_Country (or worldwide)`)))/.N
                  ), by = c('journal')]

tmp2 = dcast(melt(tmp, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable','JM','JMR','JCR','JCP','MktSci'))


# Notes:
# 
# focal variables of interest
# --> add avg. number of platforms
# add platform concentration (make clean column)
# keyword distrubtion + keyword concentration index
# Hannes checks web of science to compare author juniors
# relative impact (impact of paper, relative to all papers in the journal)
# Highest cited papers, per year + references
# Illustrative papers, highest-impact research, by year


library(knitr)
kable(tmp2,digits=3)
# -PhD/Assistant/Postdocs

# -uniq platforms / number of papers (excluding "outliers")
# - source concentration top 3 share, top 5 share (herfindahl). 
# - names of top 3 sources

```

```{r}
# JOURNAL TIME GRAPH
timing = coding[web==T, list(.N),by=c('journal', 'year')]

timing %>%
  ggplot(aes(year, N)) +
  geom_bar(stat = 'identity', aes(fill=journal)) +
  labs(caption = 
"Notes: Number of marketing research articles per year from 2004 to 2020. Included journals are Marketing Science 
(MktSci), Journal of Marketing Research (JMR), Journal of Marketing (JM), Journal of Consumer Research (JCM), 
Journal of Consumer Psychology (JCP).",
       fill = "Journal",
       x = "Year",
       y = "Number of Articles") +
  scale_fill_grey(start = 0.8, end = 0.1) +
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0.2, face = "italic"),
        axis.title = element_text(face="bold", size = "13"),
        legend.title = element_text(face="bold", size = "13"),
        axis.text.x = element_text(size="8.5")) +
  scale_x_discrete(limits=c(2004:2020))
dir.create('../../gen/analysis/output', recursive = T)
ggsave('../../gen/analysis/output/plot.png')

```

