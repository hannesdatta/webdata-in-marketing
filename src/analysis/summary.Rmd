---
title: "Summary stats"
author: "Hannes Datta"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(ggplot2)
library(stringr)
library(bibliometrix)

load(file= '../../gen/analysis/temp/citation_database.RData')
```

### Analysis
```{r}
# Overall
clean_data_source <- function(x) {
  x[grepl('googletrends', x, ignore.case=T)]<- 'Google Trends'
x[grepl('imdb', x, ignore.case=T)]<- 'IMDB'
x[grepl('amazon', x, ignore.case=T)]<- 'Amazon'
x[grepl('bn[.]com', x, ignore.case=T)]<- 'Barnes & Nobles'

  x
}
```

```{r}

tmp = coding[, list(N=length(which(web==T)), first_web_paper = min(year[web==T]),
                  avg_N = length(which(web==T))/(2020-min(year[web==T])+1),
                  author_juniors=length(which(author_assistant_professor+author_post_doc+author_phd>0))/length(which(web==T)),
                  
                  cites = mean(tcperyear[web==T]),
                  cites_index = mean(tcperyear[web==T])/mean(tcperyear),
                  cites_median = median(tcperyear[web==T]),
                  cites_median_index = median(tcperyear[web==T])/median(tcperyear),
                  
                  uniq_platforms = length(unique(str_trim(tolower(unlist(split(`scraped data_source`,',')))))),
                  share_northamerica = length(which(grepl('US|canada', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),
                  share_europe = length(which(grepl('eu|ger|uk|germany|italy|sweden|france|austria|switz|denmark|netherl|poland|luxem|belgium|czech', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  share_asia = length(which(grepl('china|korea|japan', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  share_other = length(which(grepl('worldwide|N[/]A|English[-]speaking', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  
                  
                 
                  share_textual = length(which(textual==1))/length(which(web==T)),
                  share_numeric = length(which(numeric==1))/length(which(web==T)),
                  share_images = length(which(images==1))/length(which(web==T)),
                  share_video = length(which(video==1))/length(which(web==T)),
                  
                  share_scraped = length(which(scraped==1&api==0))/length(which(web==T)),
                  share_api = length(which(api==1&scraped==0))/length(which(web==T)),
                  share_scraped_api = length(which(scraped==1&api==1))/length(which(web==T)),
                  
                  share_live = length(which(live_scraper==T))/length(which(web==T))
                  
                  #share_US_any = length(which(grepl('US', `Geography_Country (or worldwide)`)))/.N
                  ), by = c('journal')]

tmp2 = dcast(melt(tmp, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable','JM','JMR','JCR','JCP','MktSci'))


# Notes:
# 
# focal variables of interest
# --> add avg. number of platforms
# add platform concentration (make clean column)
# keyword distrubtion + keyword concentration index
# Hannes checks web of science to compare author juniors
# relative impact (impact of paper, relative to all papers in the journal)
# Highest cited papers, per year + references
# Illustrative papers, highest-impact research, by year


library(knitr)
kable(tmp2,digits=3)

# -PhD/Assistant/Postdocs

# -uniq platforms / number of papers (excluding "outliers")
# - source concentration top 3 share, top 5 share (herfindahl). 
# - names of top 3 sources

```

```{r}
# JOURNAL TIME GRAPH
timing = coding[, list(.N),by=c('journal', 'year')]

```

