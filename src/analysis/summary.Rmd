---
title: "Summary stats"
author: "Hannes Datta"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(ggplot2)
library(stringr)
library(bibliometrix)
library(dplyr)

load(file= '../../gen/analysis/temp/citation_database.RData')

```

### Analysis
```{r}
# Overall
clean_data_source <- function(x) {
  x[grepl('googletrends', x, ignore.case=T)]<- 'Google Trends'
x[grepl('imdb', x, ignore.case=T)]<- 'IMDB'
x[grepl('amazon', x, ignore.case=T)]<- 'Amazon'
x[grepl('bn[.]com', x, ignore.case=T)]<- 'Barnes & Nobles'

  x
}
```

```{r}

get_n_keywords = function(x, topx=3) {
 res=gsub('-', ' ', as.character(x))
  
 res= (str_trim(tolower(unlist(strsplit(res, ';')))))
 res=table(res)
 res=rev(res[order(res)])
 return(paste0(names(res)[1:topx], collapse=', '))
}
  
first_platforms = coding[web==T, list(platforms=unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,',')))))),
                          by = c('doi', 'year')]
first_platforms[, first:=year==min(year),by=c('platforms')]
first_platforms_agg = first_platforms[, list(first=any(first)),by=c('doi')]

setkey(first_platforms_agg, doi)
setkey(coding, doi)
coding[first_platforms_agg, first_time:=i.first]

coding[, first_web:=min(year[web==T]),by=c('journal')]

growth = rbindlist(lapply(c('first','last'), function(.half) {
  coding[, filter:=year>=first_web & year<=2012]
  if (.half=='last') coding[, filter:=year>=first_web &year>2012]
  
  gr = coding[filter==T, list(publications = .N, years = uniqueN(year),
                       publications_web = length(which(web==T)),
              first_pub=unique(first_web)),
                by=c('journal')][, half:=.half]
  return(gr)
}))
  
growth[, ':=' (avgoutput=publications/years, avgoutput_web = publications_web/years)]

gr=growth[, list(growth_all = avgoutput[half=='last']/avgoutput[half=='first'],
              growth_web = avgoutput_web[half=='last']/avgoutput_web[half=='first']), by = c('journal')]

setkey(gr, journal)
setkey(coding, journal)
coding[gr, ':=' (growth_all = i.growth_all,
                 growth_web = i.growth_web,
                 growth_index = i.growth_web/i.growth_all)]

  
#top_keywords_first= get_n_keywords(as.character(keywords_author[web==T & year <=2012]))

                  

tmp = coding[, list(N=length(which(web==T)), first_web_paper = min(year[web==T]),
                  avg_papers_per_year = length(which(web==T))/(2020-min(year[web==T])+1),
                  growth = mean(growth_web),
                  growth_index = mean(growth_index),
                  share_juniors=length(which(author_assistant_professor+author_post_doc+author_phd>0))/length(which(web==T)),
                  
                  cites_per_year = mean(tcperyear[web==T]),
                  cites_per_year_indexed = mean(tcperyear[web==T])/mean(tcperyear),
                  
                  number_platforms = length(unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,',')))))),
                  platforms_per_paper = length(unique(str_trim(tolower(unlist(strsplit(`scraped data_source`,','))))))/length(which(web==T)),
                  share_firsttime_platforms = mean(first_time[web==T]),
                  share_worldwide = length(which(grepl('worldwide', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),
                  share_northamerica = length(which(grepl('US|canada', `geography_country (or worldwide)`,ignore.case=T)))/length(which(web==T)),
                  share_europe = length(which(grepl('eu|ger|uk|germany|italy|sweden|france|austria|switz|denmark|netherl|poland|luxem|belgium|czech', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  share_asia = length(which(grepl('china|korea|japan', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  share_other = length(which(grepl('N[/]A|English[-]speaking', `geography_country (or worldwide)`, ignore.case=T)))/length(which(web==T)),
                  
                  top_keywords_first= get_n_keywords(as.character(keywords_author[web==T & year <=2012])),
                  top_keywords_late= get_n_keywords(as.character(keywords_author[web==T & year >2012])),
                  
                  
                 
                  share_textual = length(which(textual==1))/length(which(web==T)),
                  share_numeric = length(which(numeric==1))/length(which(web==T)),
                  share_images = length(which(images==1))/length(which(web==T)),
                  share_video = length(which(video==1))/length(which(web==T)),
                  share_aggregator = length(which(is_aggregator))/length(which(web==T)),
                  
                  share_scraped = length(which(scraped==1&api==0))/length(which(web==T)),
                  share_api = length(which(api==1&scraped==0))/length(which(web==T)),
                  share_scraped_api = length(which(scraped==1&api==1))/length(which(web==T)),
                  
                  share_live = length(which(live_scraper==T))/length(which(web==T))
                  
                  #share_US_any = length(which(grepl('US', `Geography_Country (or worldwide)`)))/.N
                  ), by = c('journal')]

tmp2 = dcast(melt(tmp, id.vars=c('journal')), variable~journal)
setcolorder(tmp2, c('variable','JM','JMR','JCR','JCP','MktSci'))


# Notes:
# 
# focal variables of interest
# --> add avg. number of platforms
# add platform concentration (make clean column)
# keyword distrubtion + keyword concentration index
# Hannes checks web of science to compare author juniors
# relative impact (impact of paper, relative to all papers in the journal)
# Highest cited papers, per year + references
# Illustrative papers, highest-impact research, by year


library(knitr)
kable(tmp2,digits=3)
# -PhD/Assistant/Postdocs

# -uniq platforms / number of papers (excluding "outliers")
# - source concentration top 3 share, top 5 share (herfindahl). 
# - names of top 3 sources

```

```{r}
# JOURNAL TIME GRAPH
timing = coding[web==T, list(.N),by=c('journal', 'year')]


timing %>%
  mutate(journal = factor(journal, levels = c("JCP", "JCR", "MktSci", "JMR", "JM"))) %>%
  ggplot(aes(year, N)) +
  geom_bar(stat = 'identity', aes(fill=journal)) +
  labs(#caption = 
#"Notes: Number of marketing research articles per year from 2004 to 2020. Included journals are Marketing Science 
#(MktSci), Journal of Marketing Research (JMR), Journal of Marketing (JM), Journal of Consumer Research (JCM), 
#Journal of Consumer Psychology (JCP).",
       fill = "Journal",
       x = "Year",
       y = "Number of Articles") +
  scale_fill_manual(values = c("gray30", "grey85", "gray60", "gray40", "gray1")) +
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0.2, face = "italic"),
        axis.title = element_text(face="bold", size = "13"),
        legend.title = element_text(face="bold", size = "13"),
        axis.text.x = element_text(size="8.5")) +
  scale_x_discrete(limits=c(2004:2020)) + 
  guides(fill = guide_legend(reverse=T))
dir.create('../../gen/analysis/output', recursive = T)
ggsave('../../gen/analysis/output/plot.png')

```

